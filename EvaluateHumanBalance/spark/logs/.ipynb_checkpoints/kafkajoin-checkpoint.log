Traceback (most recent call last):
  File "/data/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/data/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o100.awaitTermination.
: org.apache.spark.sql.streaming.StreamingQueryException: Writing job aborted.
=== Streaming Query ===
Identifier: [id = 72306e6e-a6f0-41a5-84e2-ac81f5fe4607, runId = b71f3c0c-a874-4246-b1f3-e1c5736ad593]
Current Committed Offsets: {}
Current Available Offsets: {KafkaV2[Subscribe[redis-server]]: {"redis-server":{"0":69}},KafkaV2[Subscribe[stedi-events]]: {"stedi-events":{"0":0}}}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
Project [cast(customer#92 as string) AS key#112, structstojson(named_struct(customer, customer#92, score, score#93, email, email#48, birthYear, birthYear#59), Some(Etc/UTC)) AS value#113]
+- Join Inner, (customer#92 = email#48)
   :- Project [customer#92, score#93]
   :  +- SubqueryAlias `customerrisk`
   :     +- Project [value#90.customer AS customer#92, value#90.score AS score#93, value#90.riskDate AS riskDate#94]
   :        +- Project [jsontostructs(StructField(customer,StringType,true), StructField(score,FloatType,true), StructField(riskDate,StringType,true), value#88, Some(Etc/UTC)) AS value#90]
   :           +- Project [cast(value#75 as string) AS value#88]
   :              +- StreamingExecutionRelation KafkaV2[Subscribe[stedi-events]], [key#74, value#75, topic#76, partition#77, offset#78L, timestamp#79, timestampType#80]
   +- Project [email#48, birthYear#59]
      +- Project [customerName#47, email#48, phone#49, birthDay#50, split(birthDay#50, -)[0] AS birthYear#59]
         +- Project [customerName#47, email#48, phone#49, birthDay#50]
            +- Filter (isnotnull(email#48) && isnotnull(birthday#50))
               +- SubqueryAlias `customerrecords`
                  +- Project [encodedCustomer#44.customerName AS customerName#47, encodedCustomer#44.email AS email#48, encodedCustomer#44.phone AS phone#49, encodedCustomer#44.birthDay AS birthDay#50]
                     +- Project [key#28, jsontostructs(StructField(customerName,StringType,true), StructField(email,StringType,true), StructField(phone,StringType,true), StructField(birthDay,StringType,true), encodedCustomer#41, Some(Etc/UTC)) AS encodedCustomer#44]
                        +- Project [key#28, cast(unbase64(encodedCustomer#38) as string) AS encodedCustomer#41]
                           +- Project [key#28, zSetEntries#32[0].element AS encodedCustomer#38]
                              +- SubqueryAlias `redissortedset`
                                 +- Project [value#25.key AS key#28, value#25.existType AS existType#29, value#25.Ch AS Ch#30, value#25.Incr AS Incr#31, value#25.zSetEntries AS zSetEntries#32]
                                    +- Project [key#21, jsontostructs(StructField(key,StringType,true), StructField(existType,StringType,true), StructField(Ch,BooleanType,true), StructField(Incr,BooleanType,true), StructField(zSetEntries,ArrayType(StructType(StructField(element,StringType,true), StructField(score,StringType,true)),true),true), value#22, Some(Etc/UTC)) AS value#25]
                                       +- Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]
                                          +- StreamingExecutionRelation KafkaV2[Subscribe[redis-server]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
Caused by: org.apache.spark.SparkException: Writing job aborted.
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$17.apply(MicroBatchExecution.scala:540)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:535)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:534)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
	... 1 more
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 2, localhost, executor driver): org.apache.kafka.common.KafkaException: Failed to construct kafka producer
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:457)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:272)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$.org$apache$spark$sql$kafka010$CachedKafkaProducer$$createKafkaProducer(CachedKafkaProducer.scala:67)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$$anon$1.load(CachedKafkaProducer.scala:46)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$$anon$1.load(CachedKafkaProducer.scala:43)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$.getOrCreate(CachedKafkaProducer.scala:80)
	at org.apache.spark.sql.kafka010.KafkaStreamDataWriter.producer$lzycompute(KafkaStreamWriter.scala:90)
	at org.apache.spark.sql.kafka010.KafkaStreamDataWriter.producer(KafkaStreamWriter.scala:90)
	at org.apache.spark.sql.kafka010.KafkaStreamDataWriter.commit(KafkaStreamWriter.scala:103)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:127)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:66)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:406)
	... 29 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:64)
	... 35 more
Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka producer
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:457)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:272)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$.org$apache$spark$sql$kafka010$CachedKafkaProducer$$createKafkaProducer(CachedKafkaProducer.scala:67)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$$anon$1.load(CachedKafkaProducer.scala:46)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$$anon$1.load(CachedKafkaProducer.scala:43)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.kafka010.CachedKafkaProducer$.getOrCreate(CachedKafkaProducer.scala:80)
	at org.apache.spark.sql.kafka010.KafkaStreamDataWriter.producer$lzycompute(KafkaStreamWriter.scala:90)
	at org.apache.spark.sql.kafka010.KafkaStreamDataWriter.producer(KafkaStreamWriter.scala:90)
	at org.apache.spark.sql.kafka010.KafkaStreamDataWriter.commit(KafkaStreamWriter.scala:103)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:127)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:66)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:406)
	... 29 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/workspace/sparkpykafkajoin.py", line 185, in <module>
    .option("checkpointLocation", "/temp/kafkacheckpoint")\
  File "/data/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 103, in awaitTermination
  File "/data/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/data/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 75, in deco
pyspark.sql.utils.StreamingQueryException: 'Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = 72306e6e-a6f0-41a5-84e2-ac81f5fe4607, runId = b71f3c0c-a874-4246-b1f3-e1c5736ad593]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[redis-server]]: {"redis-server":{"0":69}},KafkaV2[Subscribe[stedi-events]]: {"stedi-events":{"0":0}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [cast(customer#92 as string) AS key#112, structstojson(named_struct(customer, customer#92, score, score#93, email, email#48, birthYear, birthYear#59), Some(Etc/UTC)) AS value#113]\n+- Join Inner, (customer#92 = email#48)\n   :- Project [customer#92, score#93]\n   :  +- SubqueryAlias `customerrisk`\n   :     +- Project [value#90.customer AS customer#92, value#90.score AS score#93, value#90.riskDate AS riskDate#94]\n   :        +- Project [jsontostructs(StructField(customer,StringType,true), StructField(score,FloatType,true), StructField(riskDate,StringType,true), value#88, Some(Etc/UTC)) AS value#90]\n   :           +- Project [cast(value#75 as string) AS value#88]\n   :              +- StreamingExecutionRelation KafkaV2[Subscribe[stedi-events]], [key#74, value#75, topic#76, partition#77, offset#78L, timestamp#79, timestampType#80]\n   +- Project [email#48, birthYear#59]\n      +- Project [customerName#47, email#48, phone#49, birthDay#50, split(birthDay#50, -)[0] AS birthYear#59]\n         +- Project [customerName#47, email#48, phone#49, birthDay#50]\n            +- Filter (isnotnull(email#48) && isnotnull(birthday#50))\n               +- SubqueryAlias `customerrecords`\n                  +- Project [encodedCustomer#44.customerName AS customerName#47, encodedCustomer#44.email AS email#48, encodedCustomer#44.phone AS phone#49, encodedCustomer#44.birthDay AS birthDay#50]\n                     +- Project [key#28, jsontostructs(StructField(customerName,StringType,true), StructField(email,StringType,true), StructField(phone,StringType,true), StructField(birthDay,StringType,true), encodedCustomer#41, Some(Etc/UTC)) AS encodedCustomer#44]\n                        +- Project [key#28, cast(unbase64(encodedCustomer#38) as string) AS encodedCustomer#41]\n                           +- Project [key#28, zSetEntries#32[0].element AS encodedCustomer#38]\n                              +- SubqueryAlias `redissortedset`\n                                 +- Project [value#25.key AS key#28, value#25.existType AS existType#29, value#25.Ch AS Ch#30, value#25.Incr AS Incr#31, value#25.zSetEntries AS zSetEntries#32]\n                                    +- Project [key#21, jsontostructs(StructField(key,StringType,true), StructField(existType,StringType,true), StructField(Ch,BooleanType,true), StructField(Incr,BooleanType,true), StructField(zSetEntries,ArrayType(StructType(StructField(element,StringType,true), StructField(score,StringType,true)),true),true), value#22, Some(Etc/UTC)) AS value#25]\n                                       +- Project [cast(key#7 as string) AS key#21, cast(value#8 as string) AS value#22]\n                                          +- StreamingExecutionRelation KafkaV2[Subscribe[redis-server]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\n'
